---------Activation functions-------
Activation Function to use in the hidden layer as well as at the output layer of the network.
--------Elements of a Neural Network--------
Input Layer: This layer accepts input features.
It provides information from the outside world to the network, no computation is performed at this layer, nodes here just pass on the information(features) to the hidden layer. 

Hidden Layer: Nodes of this layer are not exposed to the outer world, they are part of the abstraction provided by any neural network. 
The hidden layer performs all sorts of computation on the features entered through the input layer and transfers the result to the output layer. 
-----sigmoid----
The sigmoid function transforms the continuous real number into a range of ( 0 , 1 ) 
-----------Relu--------
The ReLU activation function is used to introduce nonlinearity in a neural network, helping mitigate the vanishing gradient problem during machine learning model training and enabling neural networks to learn more complex relationships in data.
----softmax-----
The softmax function is a function that turns a vector of K real values into a vector of K real values that sum to 1.
----Tanh-----
Tanh is the hyperbolic tangent function, which is the hyperbolic analogue of the Tan circular function used throughout trigonometry.
